{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import subprocess\n",
    "import glob\n",
    "from math import floor, ceil\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from datagen import data_generator,video_to_frames\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from tensorflow.keras.layers import ConvLSTM2D, Dense, Flatten, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "pf = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(pf[0],True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE = (128,128)\n",
    "\n",
    "T = 10\n",
    "def video_to_frames(path_to_vid):\n",
    "   \n",
    "    cap = cv2.VideoCapture(path_to_vid)\n",
    "    n_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    frm_list = []\n",
    "    counter = 0\n",
    "    valid_from = ceil(0.15*n_frames)\n",
    "    valid_to = floor(0.1*n_frames)\n",
    "    num_frames = n_frames-valid_from-valid_to\n",
    "    frame_to_count = floor(num_frames/T)\n",
    "    no_of_frames_extracted = 0\n",
    "    while no_of_frames_extracted < T:\n",
    "        ret, frame = cap.read()\n",
    "        if ret==True:\n",
    "            if counter>=valid_from and (counter-valid_from)//frame_to_count==0:\n",
    "                img = cv2.resize(frame,IMAGE)\n",
    "                #gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                #np.expand_dims(gray,axis=1)\n",
    "                frm_list.append(img)\n",
    "                no_of_frames_extracted+=1\n",
    "                counter+=1\n",
    "            else:\n",
    "                counter+=1\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return frm_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "frm_list = video_to_frames('/home/tarun/Kinetic600/kinetics-downloader/dataset/train/juggling_balls/_9ZQNnjgdME.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "T=30\n",
    "IMAGE = (128,128)\n",
    "def video_to_frames(path_to_vid):\n",
    "   \n",
    "    cap = cv2.VideoCapture(path_to_vid)\n",
    "    n_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    frm_list = []\n",
    "    counter = 0\n",
    "    no_of_frames_extracted = 0\n",
    "    while no_of_frames_extracted < T:\n",
    "        ret, frame = cap.read()\n",
    "        if ret==True:\n",
    "            if counter//T==0:\n",
    "                img = cv2.resize(frame,IMAGE)\n",
    "                #gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                #np.expand_dims(gray,axis=1)\n",
    "                frm_list.append(img)\n",
    "                no_of_frames_extracted+=1\n",
    "                counter+=1\n",
    "            else:\n",
    "                counter+=1\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return frm_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "frm = np.asarray(frm_list)\n",
    "print(frm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(input_dir,input_class):\n",
    "    X = []\n",
    "    Y = []\n",
    "    no_of_videos_skipped = 0\n",
    "    videos = [vid for vid in os.listdir(input_dir) if vid.endswith('.mp4')]\n",
    "    print(\"Starting at : \",str(len(videos)))\n",
    "    for vid in videos:\n",
    "        frame_list = video_to_frames(os.path.join(input_dir,vid))\n",
    "        # print(frame_list)\n",
    "        if len(frame_list)  <  T:\n",
    "            no_of_videos_skipped+=1\n",
    "            print('Removed')\n",
    "            videos.remove(vid)\n",
    "\n",
    "        else:\n",
    "            X.append(frame_list)\n",
    "            y = [0]*2  \n",
    "            y[input_class] = 1  #one hot encoading\n",
    "            Y.append(y)\n",
    "            \n",
    "    X = np.asarray(X)\n",
    "    Y = np.asarray(Y)\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at :  694\n",
      "Starting at :  845\n",
      "Removed\n",
      "Removed\n"
     ]
    }
   ],
   "source": [
    "X1,Y1 = data_generator('/home/tarun/Kinetic600/kinetics-downloader/dataset/train/juggling_balls',0)\n",
    "X2,Y2 = data_generator('/home/tarun/Kinetic600/kinetics-downloader/dataset/train/punching_bag',1)\n",
    "\n",
    "X_train = np.concatenate((X1, X2))\n",
    "y_train = np.concatenate((Y1, Y2))\n",
    "\n",
    "X_train,y_train = shuffle(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1535, 30, 128, 128, 3)\n",
      "Starting at :  40\n",
      "Starting at :  41\n",
      "(81, 30, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "np.save('fames',X_train)\n",
    "np.save('results',y_train)\n",
    "print(X_train.shape)\n",
    "X_test1,y_test1 = data_generator('/home/tarun/Kinetic600/kinetics-downloader/dataset/valid/juggling_balls',0)\n",
    "X_test2,y_test2 = data_generator('/home/tarun/Kinetic600/kinetics-downloader/dataset/valid/punching_bag',1)\n",
    "\n",
    "X_test = np.concatenate((X_test1, X_test2))\n",
    "y_test = np.concatenate((y_test1, y_test2))\n",
    "\n",
    "X_test,y_test = shuffle(X_test,y_test)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81, 30, 3, 128, 128)\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(1535,30,3,128,128)\n",
    "X_test = X_test.reshape(81,30,3,128,128)\n",
    "print(X_test.shape)\n",
    "print(y_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 30, 3, 128, 128)] 0         \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d (ConvLSTM2D)    (None, 30, 3, 128, 64)    1229056   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 30, 3, 128, 64)    256       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 30, 3, 128, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_1 (ConvLSTM2D)  (None, 3, 128, 128)       2458112   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3, 128, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 49152)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               12583168  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 16,271,106\n",
      "Trainable params: 16,270,978\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "T = 30\n",
    "IMAGE = (64,64)\n",
    "i = Input(shape=(T, 3, 128, 128))\n",
    "\n",
    "def myConv(model):\n",
    "    model = ConvLSTM2D(filters=64, kernel_size=(5,5), padding='same',return_sequences = True)(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Dropout(0.2)(model)\n",
    "    model = ConvLSTM2D(filters=128, kernel_size=(5,5), padding='same',return_sequences = False)(model)\n",
    "    model = Dropout(0.2)(model)\n",
    "    model = Flatten()(model)\n",
    "    model = Dense(256, activation=\"relu\")(model)\n",
    "    model = Dropout(0.3)(model)\n",
    "    model = Dense(2,activation='softmax')(model)\n",
    "    \n",
    "    return model\n",
    "model = myConv(i)\n",
    "model = Model(i,model)\n",
    "opt = tf.keras.optimizers.SGD(lr=0.001)\n",
    "model.compile(optimizer= opt,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "192/192 [==============================] - 141s 737ms/step - loss: 0.6834 - accuracy: 0.5824 - val_loss: 0.7014 - val_accuracy: 0.5185\n",
      "Epoch 2/30\n",
      "192/192 [==============================] - 141s 735ms/step - loss: 0.6468 - accuracy: 0.6248 - val_loss: 0.6464 - val_accuracy: 0.6296\n",
      "Epoch 3/30\n",
      "192/192 [==============================] - 142s 738ms/step - loss: 0.6137 - accuracy: 0.6678 - val_loss: 0.6826 - val_accuracy: 0.6049\n",
      "Epoch 4/30\n",
      "192/192 [==============================] - 141s 737ms/step - loss: 0.5816 - accuracy: 0.6932 - val_loss: 0.6607 - val_accuracy: 0.6173\n",
      "Epoch 5/30\n",
      "192/192 [==============================] - 142s 738ms/step - loss: 0.5816 - accuracy: 0.6951 - val_loss: 0.6828 - val_accuracy: 0.6173\n",
      "Epoch 6/30\n",
      "192/192 [==============================] - 141s 736ms/step - loss: 0.5905 - accuracy: 0.7003 - val_loss: 0.6824 - val_accuracy: 0.6543\n",
      "Epoch 7/30\n",
      "192/192 [==============================] - 141s 736ms/step - loss: 0.5696 - accuracy: 0.6906 - val_loss: 0.6472 - val_accuracy: 0.6667\n",
      "Epoch 8/30\n",
      "192/192 [==============================] - 141s 736ms/step - loss: 0.5712 - accuracy: 0.7127 - val_loss: 0.6149 - val_accuracy: 0.6667\n",
      "Epoch 9/30\n",
      "192/192 [==============================] - 142s 738ms/step - loss: 0.5386 - accuracy: 0.7283 - val_loss: 0.5926 - val_accuracy: 0.7284\n",
      "Epoch 10/30\n",
      "192/192 [==============================] - 150s 781ms/step - loss: 0.5327 - accuracy: 0.7270 - val_loss: 0.6278 - val_accuracy: 0.6790\n",
      "Epoch 11/30\n",
      "192/192 [==============================] - 154s 800ms/step - loss: 0.5135 - accuracy: 0.7485 - val_loss: 0.7381 - val_accuracy: 0.6296\n",
      "Epoch 12/30\n",
      " 24/192 [==>...........................] - ETA: 2:02 - loss: 0.4683 - accuracy: 0.7708"
     ]
    }
   ],
   "source": [
    "r = model.fit(X_train,y_train,validation_data=(X_test, y_test),batch_size=8,epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 30, 3, 128, 128)] 0         \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_1 (ConvLSTM2D)  (None, 3, 128, 64)        1229056   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 3, 128, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 24576)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               6291712   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 7,521,282\n",
      "Trainable params: 7,521,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "192/192 [==============================] - 47s 243ms/step - loss: 0.7448 - accuracy: 0.5192 - val_loss: 0.6845 - val_accuracy: 0.5926\n",
      "Epoch 2/30\n",
      "192/192 [==============================] - 47s 243ms/step - loss: 0.6865 - accuracy: 0.5700 - val_loss: 0.6879 - val_accuracy: 0.5309\n",
      "Epoch 3/30\n",
      "192/192 [==============================] - 47s 244ms/step - loss: 0.6572 - accuracy: 0.6078 - val_loss: 0.7404 - val_accuracy: 0.5062\n",
      "Epoch 4/30\n",
      "192/192 [==============================] - ETA: 0s - loss: 0.6424 - accuracy: 0.6261"
     ]
    }
   ],
   "source": [
    "T = 30\n",
    "IMAGE = (64,64)\n",
    "i = Input(shape=(T, 3, 128, 128))\n",
    "x = ConvLSTM2D(filters=64, kernel_size=(5,5), padding='same',return_sequences = False)(i)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = ConvLSTM2D(filters=128, kernel_size=(5,5), padding='same',return_sequences = False)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(2,activation='softmax')(x)\n",
    "opt = tf.keras.optimizers.SGD(lr=0.001)\n",
    "model = Model(i,x)\n",
    "model.compile(optimizer= opt,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "r = model.fit(X_train,y_train,validation_data=(X_test, y_test),batch_size=8,epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linked TensorRT version (6, 0, 1)\n",
      "Loaded TensorRT version (6, 0, 1)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.compiler.tf2tensorrt.wrap_py_utils import get_linked_tensorrt_version\n",
    "from tensorflow.compiler.tf2tensorrt.wrap_py_utils import get_loaded_tensorrt_version\n",
    "\n",
    "print(f\"Linked TensorRT version {get_linked_tensorrt_version()}\")\n",
    "print(f\"Loaded TensorRT version {get_loaded_tensorrt_version()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
